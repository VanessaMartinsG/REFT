{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Criando o Modelo REF (Reconhecimento de Emoções por Fala)**"
      ],
      "metadata": {
        "id": "umdfJidp9nmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conectando ao drive para:\n",
        "- Ter acesso a pasta de espectrogramas\n",
        "- Salvar o modelo"
      ],
      "metadata": {
        "id": "P2VfXQgxAGR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUbtR1YB9sIE",
        "outputId": "87f6d968-fe82-4a7c-e6b2-543b447d3cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação do modelo utilizando Resnet34\n",
        "\n",
        "*Resnet34 é um modelo de classificação de imagens de última geração, estruturado como uma rede neural convolucional de 34 camadas e definido em \" Deep Residual Learning for Image Recognition \". Restnet34 é pré-treinado no conjunto de dados ImageNet que contém mais de 100.000 imagens em 200 classes diferentes.*\n",
        "\n",
        "*No entanto, RestNet é diferente das redes neurais tradicionais no sentido de que pega resíduos de cada camada e os utiliza nas camadas conectadas subsequentes (semelhante às redes neurais residuais usadas para previsão de texto).*"
      ],
      "metadata": {
        "id": "LYb0ANEGAZcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Defina as classes de emoções\n",
        "EMOTIONS = [\"felicidade\", \"tristeza\", \"raiva\", \"neutro\", \"medo\"]\n",
        "\n",
        "# Crie uma classe personalizada para o conjunto de dados\n",
        "class AudioEmotionDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []  # Lista para armazenar pares (imagem, rótulo)\n",
        "\n",
        "        # Carregar imagens e atribuir rótulos\n",
        "        for i, emotion in enumerate(EMOTIONS):\n",
        "            emotion_dir = os.path.join(root_dir, emotion)\n",
        "            for filename in os.listdir(emotion_dir):\n",
        "                image_path = os.path.join(emotion_dir, filename)\n",
        "                self.data.append((image_path, i))  # (imagem, rótulo)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.data[idx]\n",
        "        image = plt.imread(image_path)  # Carregar imagem\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Defina transformações de dados\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Crie conjuntos de treinamento e validação\n",
        "train_dataset = AudioEmotionDataset(\"/content/drive/MyDrive/espectrogramas\", transform=transform)\n",
        "\n",
        "# Use train-test split para separar um subconjunto para validação\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Defina os dataloaders para treinamento e validação\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# Crie o modelo ResNet-34 e defina a função de perda e otimizador\n",
        "model = models.resnet34(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, len(EMOTIONS))  # Camada de saída com número de classes de emoções\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Treine o modelo\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        predicted_labels = []\n",
        "        true_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                predicted_labels.extend(predicted.cpu().numpy())\n",
        "                true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
        "\n",
        "    return model, true_labels, predicted_labels\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model, true_labels, predicted_labels = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
        "\n",
        "# Avalie o modelo\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(true_labels, predicted_labels, target_names=EMOTIONS))\n",
        "\n",
        "# Matriz de confusão\n",
        "confusion = confusion_matrix(true_labels, predicted_labels)\n",
        "print(\"Matriz de Confusão:\")\n",
        "print(confusion)\n",
        "\n",
        "# Salve o modelo treinado\n",
        "torch.save(model.state_dict(), \"/content/drive/My Drive/audio_emotion_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oZSfWZg9wIr",
        "outputId": "57f18b4e-4589-40b4-d588-f6bfebee94ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 132MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:152: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.6725662776402064, Val Loss: 1.6553897857666016\n",
            "Epoch 2/10, Loss: 1.5146723645074027, Val Loss: 1.598442256450653\n",
            "Epoch 3/10, Loss: 1.4058803490230016, Val Loss: 1.5615379214286804\n",
            "Epoch 4/10, Loss: 1.2562792982373918, Val Loss: 1.4866124987602234\n",
            "Epoch 5/10, Loss: 1.1359678847449166, Val Loss: 1.4363253712654114\n",
            "Epoch 6/10, Loss: 1.0224775416510445, Val Loss: 1.4235925078392029\n",
            "Epoch 7/10, Loss: 0.8819666675158909, Val Loss: 1.372149407863617\n",
            "Epoch 8/10, Loss: 0.7860389692442757, Val Loss: 1.359984815120697\n",
            "Epoch 9/10, Loss: 0.6729327866009304, Val Loss: 1.3645009994506836\n",
            "Epoch 10/10, Loss: 0.5345360807010106, Val Loss: 1.3286758661270142\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  felicidade       0.75      0.90      0.82        10\n",
            "    tristeza       0.60      0.50      0.55        12\n",
            "       raiva       0.59      0.77      0.67        13\n",
            "      neutro       0.50      0.27      0.35        11\n",
            "        medo       0.33      0.38      0.35         8\n",
            "\n",
            "    accuracy                           0.57        54\n",
            "   macro avg       0.55      0.56      0.55        54\n",
            "weighted avg       0.57      0.57      0.56        54\n",
            "\n",
            "Matriz de Confusão:\n",
            "[[ 9  0  1  0  0]\n",
            " [ 0  6  1  1  4]\n",
            " [ 0  1 10  1  1]\n",
            " [ 1  3  3  3  1]\n",
            " [ 2  0  2  1  3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "LZnzRemknCxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reconhecendo a emoção\n",
        "\n",
        "Com o modelo criado, agora chegou a hora de testarmos o reconhecimento de emoções passando um áudio como referência.\n",
        "\n",
        "*OBS.: O ideal é que seja um áudio fora do escopo de treinamento e validação do modelo*"
      ],
      "metadata": {
        "id": "S9wIy8gIBRuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Verifique se a GPU está disponível e use-a para inferência, se possível\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define o dicionário de emoções\n",
        "EMOTIONS = {\n",
        "    0: \"felicidade\",\n",
        "    1: \"medo\",\n",
        "    2: \"neutro\",\n",
        "    3: \"raiva\",\n",
        "    4: \"tristeza\"\n",
        "}\n",
        "\n",
        "# Carregue o modelo treinado\n",
        "model = models.resnet34(pretrained=False)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, len(EMOTIONS))  # Ajuste o número de classes\n",
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/audio_emotion_model.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define a função de predição\n",
        "def predict_emotion(audio_path):\n",
        "    # Recebe um áudio\n",
        "    audio, sampling_rate = librosa.load(audio_path)\n",
        "\n",
        "    # Transforma o áudio em um espectrograma\n",
        "    spectrogram = librosa.stft(audio, n_fft=512, hop_length=256)\n",
        "    spectrogram_magnitude = np.abs(spectrogram)\n",
        "    spectrogram_magnitude = spectrogram_magnitude.astype(np.float32)\n",
        "\n",
        "    # Transforma o espectrograma em um tensor PyTorch\n",
        "    spectrogram_tensor = torch.from_numpy(spectrogram_magnitude)\n",
        "    spectrogram_tensor = spectrogram_tensor.unsqueeze(0).repeat(1, 3, 1, 1)\n",
        "\n",
        "    # Move o tensor de entrada para a GPU\n",
        "    spectrogram_tensor = spectrogram_tensor.to(device)\n",
        "\n",
        "    # Faz a predição da emoção\n",
        "    outputs = model(spectrogram_tensor)\n",
        "\n",
        "    # Retorna a emoção predita\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    predicted_emotion = EMOTIONS[predicted.item()]\n",
        "\n",
        "    return predicted_emotion\n",
        "\n",
        "\n",
        "# Exemplo de uso\n",
        "predicted_emotion1 = predict_emotion(\"medo.wav\")\n",
        "predicted_emotion2 = predict_emotion(\"tristeza.wav\")\n",
        "predicted_emotion3 = predict_emotion(\"felicidade.wav\")\n",
        "predicted_emotion4 = predict_emotion(\"neutro.wav\")\n",
        "predicted_emotion5 = predict_emotion(\"raiva.wav\")\n",
        "\n",
        "print(predicted_emotion1)\n",
        "print(predicted_emotion2)\n",
        "print(predicted_emotion3)\n",
        "print(predicted_emotion4)\n",
        "print(predicted_emotion5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRDh4IuTnIjP",
        "outputId": "4ce46c2b-cd24-41d9-b1d8-52eecfa49862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutro\n",
            "tristeza\n",
            "neutro\n",
            "neutro\n",
            "raiva\n"
          ]
        }
      ]
    }
  ]
}